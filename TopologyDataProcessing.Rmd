---
title: "TopologyProjectDataProcessing"
author: "Kevin Shin"
date: "4/24/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidytext)
library(dplyr)
library(stringr)
library(tokenizers)
library(gutenbergr)
library(janeaustenr)
library(rapportools)
library(textstem)
```
##Clean Data set

```{r}
gutenberg_filtered <- gutenberg_metadata %>% filter(is.na(author)==FALSE) %>% filter(is.na(title)==FALSE) %>% filter(has_text == TRUE) %>%  filter(rights == "Public domain in the USA.") %>% filter(language == "en") %>% filter(author != "Various") %>% filter(author != "Anonymous")
```

```{r}
gutenberg_count <- count(gutenberg_filtered, author)
relevant_authors <- gutenberg_count %>% filter(n >= 10)
```

###Example

####David Hume Example
```{r}
David_Hume <- gutenberg_filtered %>% filter(author == "Hume, David")
Hume_Works <- gutenberg_download(David_Hume$gutenberg_id,strip=TRUE)
```
```{r}
book_1257 <- gutenberg_download(1257,strip=TRUE)
View(book_1257)
book_1257_paragraph <- unnest_tokens(book_1257,input="text",output="Paragraph",token="paragraphs")
View(book_1257_paragraph)
```

####Dictionary
```{r}
dictionary <- Hume_Works %>% unnest_tokens(word, text)
```

```{r}
#cleanDictionary <- transmute(dictionary,word = gsub('[0-9]+', '', str_replace_all(word,"[[:punct:] ]+","")))
cleanDictionary <- transmute(dictionary,word = str_replace_all(word,"([[:punct:]]+|[0-9]+)",""))
cleanDictionary <- cleanDictionary %>% filter(word != "") %>% filter(word != ".")
```

```{r}
uniqueWords <- unique(cleanDictionary, incomparables = FALSE)
uniqueLemma <- lemmatize_words(uniqueWords$word)
uniqueLemma <- unique(uniqueLemma, incomparables = FALSE)

View(uniqueWords)
View(uniqueLemma)

uniqueLemma <- data.frame("word" = uniqueLemma)
uniqueLemma <- transmute(uniqueLemma, word = str_replace_all(word,"[0-9]+",""))
uniqueLemma <- uniqueLemma %>% filter(word != "")

```

####A particular book
```{r}
Book_4320 <- Hume_Works %>% filter(gutenberg_id == 4320)
Book_4320 <- mutate(Book_4320, LineNumber=seq.int(nrow(Book_4320)))
Book_4320 <- Book_4320 %>% filter(LineNumber >= 97)
```


```{r}
book_words <- Book_4320 %>%
  unnest_tokens(word, text) %>%
  count(LineNumber, word, sort = TRUE)

book_words <- data.frame("LineNumber" = book_words$LineNumber, "word" = lemmatize_words(book_words$word), "n" = book_words$n)

book_words <- book_words %>%
     bind_tf_idf(word, LineNumber, n)

View(book_words)

```

###main function
```{r}
#turns each line into a vector (toReturn <- 25,679 dimensions) which has a tf_idf score associated with the words that appear in the line

#tested on small subsets of lines, should work I think. 
testCase <- function(){
  #for (value in sort(unique(book_words_filtered$LineNumber))) 
  uniqueLemma4320 <- uniqueLemma
  for (value in c(97:98))   {
    lineSet <- book_words %>% filter(LineNumber == value)
    #merged <- merge(x=uniqueLemma4320,y=lineSet,by="word",all=TRUE)
    uniqueLemma4320[toString(value)] <- NA
    for (word in lineSet$word){
      uniqueLemma4320[word] = 3
    }
  }
}

testCase()

```


TO DO: 
Each book (gutenberg_id) generates a point cloud. Repeat this process for multiple books, so that you get the relevant dictionary. Making the point clouds should take some time computationally but be rather simple. If in book, include as a column.


Analysis: Ideally, there should be some "trend" of adding more cycles, subtracting more, etc.
